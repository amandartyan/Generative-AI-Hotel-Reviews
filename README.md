# Generative AI
## Generated text from Tripadvisor Hotel Reviews using GPT-2 Model

GPT-2 (Generative Pre-trained Transformer 2) is a large-scale language model developed by OpenAI, designed to generate coherent and contextually relevant text. It is the successor to GPT (Generative Pre-trained Transformer) and is built upon the Transformer architecture introduced by Vaswani et al. in 2017.

1. Data Preprocessing

   The dataset used is a collection of hotel reviews from Tripadvisor, sourced from Kaggle. This dataset was chosen because it contains diverse text, covering a wide range of topics related to the hospitality industry.
   https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews
   
3. Fine Tuning Model GPT-2
   - Huggingface Transformers
   - Tokenizer and Model
   - Model Architecture
   - Training Function
     
4. Evaluate Model
   
   Using Perplexity to measure how well predict the text 
