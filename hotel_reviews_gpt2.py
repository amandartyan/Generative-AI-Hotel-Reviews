# -*- coding: utf-8 -*-
"""Hotel reviews_GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tk0mOL3wA5QGNVCz_awMt4KLXNbXL1QC
"""

import pandas as pd

"""#Data Processing"""

dataset = pd.read_csv('tripadvisor_hotel_reviews.csv')

dataset

dataset.info()

dataset.isna().sum()

"""#Fine Tuning

"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments

pip install datasets

from datasets import Dataset

"""## prepare data for huggingface"""

data_hf = Dataset.from_pandas(dataset[['Review']].rename(columns={'Review':'Text'}))

"""## load tokenizer and model

"""

model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

max_length = 50  # text maximum

def tokenize_function(example):
  return tokenizer(
    example ['Text'],
    padding=True,           # activate padding
    truncation=True,        # truncate text longer than max_length
    max_length=max_length,  # maximum length token
    return_tensors="pt"     # return PyTorch tensors
)

tokenized_data = data_hf.map(tokenize_function, batched=True)
print("Tokenized input IDs:")
print(tokenized_data["input_ids"][0])

"""## model architecture"""

import torch
from torch import nn

class GPT2Classifier(nn.Module):
  def __init__(self, n_classes, dropout=0.2):
    super().__init__()

    self.gpt2 = GPT2LMHeadModel.from_pretrained(model_name)
    self.dropout = nn.Dropout(dropout)
    self.classifier = nn.Linear(768, n_classes)

  def forward(self, input_ids, attention_mask):
    outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)
    hidden_state = outputs.last_hidden_state
    cls_output = hidden_state[:, -1, :]

    x = self.dropout(cls_output)
    logits = self.classifier(x)

    return logits

"""## Training Function"""

from torch.utils.data import Dataset

## using Huggingface trainer API

texts = [
    "This movie is great!",
    "I love this hotel",
    "Not bad, could be better"
]

labels = [1,1,0] #1= positive, 0=negative

#Custom Dataset
class TextDataset(Dataset):
  def __init__(self, texts, labels, tokenizer, max_length=50):
    self.texts = texts,
    self.labels = labels,
    self.tokenizer= tokenizer,
    self.max_length = max_length

  def __len__(self):
    return len(self.texts)

  def __getitem__(self,idx):
    Text = self.texts[idx],
    Label = self.labels[idx]

    encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
    input_ids = encoding["input_ids"].squeeze(0)
    attention_mask = encoding["attention_mask"].squeeze(0)
    return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": input_ids  # For GPT-2, labels are the same as input_ids
        }

# Prepare dataset
dataset = TextDataset(texts = texts, tokenizer = tokenizer, labels=labels)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./result",
    evaluation_strategy="no",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=100,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=10,
    learning_rate=5e-5

)

#Training Initialization
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data,
    tokenizer=tokenizer
)

#train the model
trainer.train()

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from torch.utils.data import Dataset

# Sample texts
texts = [
    "This movie is great!",
    "I love this hotel",
    "Not bad, could be better"
]

# Load tokenizer and model
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Add special tokens if needed
tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))

# Custom Dataset Class
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].squeeze(0)
        attention_mask = encoding["attention_mask"].squeeze(0)
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": input_ids  # For GPT-2, labels are the same as input_ids
        }

# Prepare dataset
dataset = TextDataset(texts=texts, tokenizer=tokenizer)

# Training Arguments
training_args = TrainingArguments(
    output_dir="./result",
    evaluation_strategy="no",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=100,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=10,
    learning_rate=5e-5,
    report_to="none",  # Disable reporting for simplicity
    prediction_loss_only=True  # Simplify output
)

# Trainer Initialization
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    tokenizer=tokenizer
)

# Train the model
trainer.train()

"""# Evaluate Model"""

pip install evaluate

import evaluate
import math

"""## Perplexity"""

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


perplexity_metric = evaluate.load("perplexity")


texts = [
    "This movie is great!",
    "I love this hotel",
    "Not bad, could be better"
]

results = perplexity_metric.compute(
    predictions=texts,
    model_id="gpt2"
)

print(results)

"""# Model Testing"""

input_text = "The hotel is good"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate text
output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)
print(tokenizer.decode(output[0], skip_special_tokens=True))

input_text = "Not bad location hotel"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate text
output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)
print(tokenizer.decode(output[0], skip_special_tokens=True))

input_text = "Beautiful view, dissapointing service"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Generate text
output = model.generate(input_ids, max_length=50, num_return_sequences=1, temperature=0.7)
print(tokenizer.decode(output[0], skip_special_tokens=True))

